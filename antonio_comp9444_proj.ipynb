{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "35f43549-b527-4675-be91-eb86d1d55ce5",
      "metadata": {
        "id": "35f43549-b527-4675-be91-eb86d1d55ce5"
      },
      "source": [
        "### 1 - SETUP\n",
        "Install dependencies and import all necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "284d053d-c6b4-4300-9f30-cd9aeb069c09",
      "metadata": {
        "id": "284d053d-c6b4-4300-9f30-cd9aeb069c09"
      },
      "outputs": [],
      "source": [
        "#install dependencies\n",
        "#specify version because latest version returns extra unwanted output\n",
        "!pip install gym-super-mario-bros==7.4.0 gym==0.25.2 nes_py\n",
        "!pip install tensordict torchrl\n",
        "!apt-get install freeglut3 freeglut3-dev mesa-common-dev\n",
        "!pip install gym pyvirtualdisplay\n",
        "!apt-get install -y xvfb x11-utils\n",
        "!pip install stable-baselines3\n",
        "!pip install 'shimmy>=0.2.1'\n",
        "\n",
        "#ref https://github.com/yfeng997/MadMario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "de3b5e4f-2fd2-4a92-9c73-1d4a00c4b27f",
      "metadata": {
        "id": "de3b5e4f-2fd2-4a92-9c73-1d4a00c4b27f"
      },
      "outputs": [],
      "source": [
        "'''Import all modules'''\n",
        "\n",
        "#Import modules and methods to view and interact with the game\n",
        "import gym\n",
        "import gym_super_mario_bros # Gym is an OpenAI toolkit for RL\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack, RecordEpisodeStatistics, RecordVideo, GrayScaleObservation, ResizeObservation\n",
        "from nes_py.wrappers import JoypadSpace # NES Emulator for OpenAI Gym\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT # simplified controls\n",
        "from pyvirtualdisplay import Display #To display the game\n",
        "\n",
        "#Import stablebaselines, a module with superior performance to openai\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "from stable_baselines3 import PPO\n",
        "# Import Base Callback for saving models\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "#Import machine learning and neural network modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "import numpy as np\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
        "\n",
        "#Import other miscellaneous modules\n",
        "from PIL import Image\n",
        "import random, datetime, os, copy\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import os\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Eliminate warning messages\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 - DEFINITIONS:"
      ],
      "metadata": {
        "id": "ylm3VXBdtlhc"
      },
      "id": "ylm3VXBdtlhc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agent:** The attribute within the environment which interacts with it.\n",
        "\n",
        "**Environment:** The world that an agent interacts with and learns from.\n",
        "\n",
        "**Action** $a$: How the Agent responds within the Environment. The\n",
        "set of all possible Actions is called *action-space*.\n",
        "\n",
        "**State** $s$ : The agent's current position within Environment. The\n",
        "set of all possible States the Environment can be in is called\n",
        "*state-space (S)* where: $$s \\in S$$\n",
        "\n",
        "**Reward** $r$ : Reward is the feedback that the Agent receives from the Environment resulting from a particular action. An aggregation of rewards over multiple time steps is called\n",
        "**Return**, which is often discounted and denoted by: $$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma ^ 2 R_{t+3} + ... $$\n",
        "\n",
        "**Policy ($\\pi$):** The 'ruleset' followed by the Agent in its quest to find the optimal reward.\n",
        "\n",
        "**Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected\n",
        "return if you start in state $s$, take an arbitrary action\n",
        "$a$, and then for each future time step take the action that\n",
        "maximizes returns under the optimal policy ($\\pi^*$). $Q$ can be said to stand for the “quality” of\n",
        "the action in a state. We try to approximate this function."
      ],
      "metadata": {
        "id": "GxesffaCt4Bu"
      },
      "id": "GxesffaCt4Bu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 - SETUP THE ENVIRONMENT\n",
        "In Mario, the environment consists of interactable tubes, mushrooms, coins and enemies.\n",
        "\n",
        "When the agent (Mario) makes an action, we retrieve the changed (next) state, reward, boolean game status (done or not done), .\n"
      ],
      "metadata": {
        "id": "nxD8-UB4Hbbq"
      },
      "id": "nxD8-UB4Hbbq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup game, using downsampled standard (v1)\n",
        "# from https://pypi.org/project/gym-super-mario-bros/\n",
        "\n",
        "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-v1\", new_step_api=True)\n",
        "else:\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-v1\", render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "# Limit the action-space to\n",
        "#   0. walk right\n",
        "#   1. jump right\n",
        "#   legal moves: https://github.com/Kautenja/gym-super-mario-bros/blob/master/gym_super_mario_bros/actions.py\n",
        "\n",
        "LIMITED_ACTIONS = [[\"right\"], [\"right\", \"A\"]]\n",
        "env = JoypadSpace(env, LIMITED_ACTIONS)\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"Current reward: {reward}\\n Is the process done? {done}, because Time: {info['time']}\\nand Flag reached? ... is {info['flag_get']} \\n Coins: {info['coins']}\\n Score: {info['score']}\\n\")\n",
        "print(f\"Current position in (x,y) coordinates: ({info['x_pos']}, {info['y_pos']})\")\n",
        "print(f\"Currently in game stage {info['stage']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLBxC8qiHZZW",
        "outputId": "0c8d5fc0-789c-4f19-84dd-86cba8013aab"
      },
      "id": "oLBxC8qiHZZW",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current reward: 0.0\n",
            " Is the process done? False, because Time: 400\n",
            "and Flag reached? ... is False \n",
            " Coins: 0\n",
            " Score: 0\n",
            "\n",
            "Current position in (x,y) coordinates: (40, 79)\n",
            "Currently in game stage 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "69d3a155-e69d-49e6-a14b-3c04c1757486",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69d3a155-e69d-49e6-a14b-3c04c1757486",
        "outputId": "00f48894-15b6-44a7-c08a-761be3ec9bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The game is played on a 240 by 256 pixel space, with 3 color maps - RGB\n"
          ]
        }
      ],
      "source": [
        "print(f'The game is played on a {env.observation_space.shape[0]} by {env.observation_space.shape[1]} pixel space, with {env.observation_space.shape[2]} color maps - RGB')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0a50f4a-b0cc-4c2c-90d4-dde0e6cb2ffa",
      "metadata": {
        "id": "d0a50f4a-b0cc-4c2c-90d4-dde0e6cb2ffa"
      },
      "source": [
        "#### 1.2 - SHOW ACTION & OBSERVATION SPACE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "958fb8e6-eb43-472a-b4e3-15fff95a5c9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "958fb8e6-eb43-472a-b4e3-15fff95a5c9a",
        "outputId": "e9647381-b699-4947-9ace-d7ccbd190cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 possible actions, they are:\n",
            "['right'] ['right', 'A']\n"
          ]
        }
      ],
      "source": [
        "#REDUCE THIS TO JUST RIGHT + B (JUMP) AND NO ACTION - ACTION SPACE FROM 7 TO 2?\n",
        "print(f'There are {len(LIMITED_ACTIONS)} possible actions, they are:')\n",
        "print(*LIMITED_ACTIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7699c9-d75b-4779-8024-6915347121ec",
      "metadata": {
        "id": "4c7699c9-d75b-4779-8024-6915347121ec"
      },
      "source": [
        "### 2 - PREPROCESS THE ENVIRONMENT\n",
        "\n",
        "Environment data is returned to the agent in ``next_state``. As you saw\n",
        "above, each state is represented by a ``[3, 240, 256]`` size array.\n",
        "That is more information than our agent needs; for instance,\n",
        "Mario’s actions do not depend on the color of the pipes or the sky!\n",
        "\n",
        "We use **Wrappers** to preprocess environment data before sending it to\n",
        "the agent.\n",
        "\n",
        "``GrayScaleObservation`` is a common wrapper to transform an RGB image\n",
        "to grayscale; doing so reduces the size of the state representation\n",
        "without losing useful information. Now the size of each state:\n",
        "``[1, 240, 256]``\n",
        "\n",
        "``ResizeObservation`` downsamples each observation into a square image.\n",
        "New size: ``[1, 84, 84]``\n",
        "\n",
        "``SkipFrame`` is a custom wrapper that inherits from ``gym.Wrapper`` and\n",
        "implements the ``step()`` function. Because consecutive frames don’t\n",
        "vary much, we can skip n-intermediate frames without losing much\n",
        "information. The n-th frame aggregates rewards accumulated over each\n",
        "skipped frame.\n",
        "\n",
        "``FrameStack`` is a wrapper that allows us to condense consecutive frames\n",
        "of the environment into a single observation point to feed to our\n",
        "learning model. This way, we can identify if Mario was landing or\n",
        "jumping based on the direction of his movement given several of the previous\n",
        "frames.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''CONVERT TO GRAYSCALE'''\n",
        "env = GrayScaleObservation(env, keep_dim=True)\n",
        "\n",
        "'''RESIZE TO REDUCE COMPUTATION'''\n",
        "env = ResizeObservation(env, shape=(84, 84))\n",
        "\n",
        "'''SKIP FRAMES TO REDUCE INPUT SIZE'''\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, trunc, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, trunc, info\n",
        "\n",
        "env = SkipFrame(env, skip=4)\n",
        "\n",
        "'''DUMMY VECTOR ENVIRONMENT WRAPPING TO MAKE TRAINING MORE EFFICIENT VIA PARALLELISM'''\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "'''STACK MOST RECENT n-FRAMES'''\n",
        "env = VecFrameStack(env, 4, channels_order='last')"
      ],
      "metadata": {
        "id": "nhphTGClMydV"
      },
      "id": "nhphTGClMydV",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying the above wrappers to the environment, the final wrapped\n",
        "state consists of 4 gray-scaled consecutive frames stacked together, as\n",
        "shown above in the image on the left. Each time Mario makes an action,\n",
        "the environment responds with a state of this structure. The structure\n",
        "is represented by a 3-D array of size ``[4, 84, 84]``."
      ],
      "metadata": {
        "id": "6_3ofxLzOhdW"
      },
      "id": "6_3ofxLzOhdW"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W1KPT7xUn7dw"
      },
      "id": "W1KPT7xUn7dw"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o8WiW1Fsn7hp"
      },
      "id": "o8WiW1Fsn7hp"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wfH__TT7n7kB"
      },
      "id": "wfH__TT7n7kB"
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "state, reward, done, info = env.step([5])\n",
        "plt.figure(figsize=(20,16))\n",
        "for idx in range(state.shape[3]):\n",
        "    plt.subplot(1,4,idx+1)\n",
        "    plt.imshow(state[0][:,:,idx])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZNGudKmFdx8Z"
      },
      "id": "ZNGudKmFdx8Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ko4g9Z_cnq7M"
      },
      "id": "Ko4g9Z_cnq7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - CONFIGURE THE AGENT\n",
        "\n",
        "We create a class ``Mario`` to represent our agent in the game who:\n",
        "\n",
        "(a) **Acts** according to the optimal action policy based on the current\n",
        "   state (of the environment).\n",
        "\n",
        "(b) **Remembers** experiences. Experience = (current state, current\n",
        "   action, reward, next state). Mario *caches* and later *recalls* his\n",
        "   experiences to update his action policy.\n",
        "\n",
        "(c)  **Learns** a better action policy over time"
      ],
      "metadata": {
        "id": "C9_Qdo1WY3Bn"
      },
      "id": "C9_Qdo1WY3Bn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 - CREATE THE MARIO AGENT CLASS\n",
        "\n",
        "##### (a) Act Function\n",
        "\n",
        "For any given state, an agent can choose the most optimal action\n",
        "(**exploit**) or a random action (**explore**).\n",
        "\n",
        "Mario randomly explores with a chance of ``self.exploration_rate``; when\n",
        "he chooses to exploit, he relies on ``MarioNet`` (implemented in\n",
        "``Learn`` section) to provide the most optimal action.\n",
        "\n",
        "##### (b) Cache & Recall Function\n",
        "\n",
        "``cache()``: Each time Mario performs an action, he stores the\n",
        "``experience`` to his memory. His experience includes the current\n",
        "*state*, *action* performed, *reward* from the action, the *next state*,\n",
        "and whether the game is *done*.\n",
        "\n",
        "``recall()``: Mario randomly samples a batch of experiences from his\n",
        "memory, and uses that to learn the game."
      ],
      "metadata": {
        "id": "Lt2LHn1RauWM"
      },
      "id": "Lt2LHn1RauWM"
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None, epsilon = 1, e_rate_decay = 9.9995e-1,\\\n",
        "                 e_rate_min = 0.1, batch_size = 32, gamma = 0.9, learning_rate = 0.00025, optimiser = 'Adam', loss = 'L1'):\n",
        "\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.burnin = 1e4  # min. experiences before training\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "        if checkpoint:\n",
        "            self.load(checkpoint)\n",
        "\n",
        "        #STATE SPACE\n",
        "        self.state_dim = state_dim\n",
        "\n",
        "        #ACTION SPACE\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        '''REFERENCE TO THE CNN ARCHITECTURE'''\n",
        "        super().__init__(state_dim, action_dim, save_dir, checkpoint)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        self.net = self.net.to(device=self.device)\n",
        "\n",
        "        '''TO STORE PREVIOUS STATES IN A REPLAY BUFFER DICTIONARY'''\n",
        "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        '''HYPERPARAMETERS TO TUNE'''\n",
        "        self.batch_size = batch_size  #NUMBER OF PREVIOUS STATES TO RECALL FROM MEMORY\n",
        "        self.gamma = gamma #FOR TD PREDICTION\n",
        "\n",
        "        self.exploration_rate = epsilon\n",
        "        self.exploration_rate_decay = e_rate_decay\n",
        "        self.exploration_rate_min = e_rate_min\n",
        "\n",
        "        self.save_every = 5e5  # no. of experiences between saving MarioNet\n",
        "\n",
        "        if optimiser == 'Adam':\n",
        "          self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
        "        if loss == 'L1':\n",
        "          self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Given a state, choose an epsilon-greedy action and update value of step.\n",
        "\n",
        "        Inputs:\n",
        "        state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
        "        Outputs:\n",
        "        ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
        "        \"\"\"\n",
        "        # EXPLORE\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            #SELECT RANDOM ACTION\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        Inputs:\n",
        "        state (``LazyFrame``),\n",
        "        next_state (``LazyFrame``),\n",
        "        action (``int``),\n",
        "        reward (``float``),\n",
        "        done(``bool``))\n",
        "        \"\"\"\n",
        "        def first_if_tuple(x):\n",
        "            return x[0] if isinstance(x, tuple) else x\n",
        "        state = first_if_tuple(state).__array__()\n",
        "        next_state = first_if_tuple(next_state).__array__()\n",
        "\n",
        "        state = torch.tensor(state)\n",
        "        next_state = torch.tensor(next_state)\n",
        "        action = torch.tensor([action])\n",
        "        reward = torch.tensor([reward])\n",
        "        done = torch.tensor([done])\n",
        "\n",
        "        # self.memory.append((state, next_state, action, reward, done,))\n",
        "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
        "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
        "\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "\n",
        "    '''SAVE AND LOAD CHECKPOINTS'''\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
        "\n",
        "    def load(self, load_path):\n",
        "        if not load_path.exists():\n",
        "            raise ValueError(f\"{load_path} does not exist\")\n",
        "\n",
        "        ckp = torch.load(load_path, map_location=(self.device))\n",
        "        exploration_rate = ckp.get('exploration_rate')\n",
        "        state_dict = ckp.get('model')\n",
        "\n",
        "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
        "        self.net.load_state_dict(state_dict)\n",
        "        self.exploration_rate = exploration_rate\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ],
      "metadata": {
        "id": "FPgYAp66Y3PT"
      },
      "id": "FPgYAp66Y3PT",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.2 - INITIALISE THE MARIO AGENT"
      ],
      "metadata": {
        "id": "W8hkFi3PaqgP"
      },
      "id": "W8hkFi3PaqgP"
    },
    {
      "cell_type": "code",
      "source": [
        "mario_agent = Mario()"
      ],
      "metadata": {
        "id": "ONLeBQc7Y3SE"
      },
      "id": "ONLeBQc7Y3SE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 - CREATE MARIONET, THE CNN ARCHITECTURE\n",
        "\n",
        "Mario uses the [DDQN algorithm](https://arxiv.org/pdf/1509.06461)_\n",
        "under the hood. DDQN uses two ConvNets - $Q_{online}$ and\n",
        "$Q_{target}$ - that independently approximate the optimal\n",
        "action-value function.\n",
        "\n",
        "In our implementation, we share feature generator ``features`` across\n",
        "$Q_{online}$ and $Q_{target}$, but maintain separate FC\n",
        "classifiers for each. $\\theta_{target}$ (the parameters of\n",
        "$Q_{target}$) is frozen to prevent updating by backprop. Instead,\n",
        "it is periodically synced with $\\theta_{online}$ (more on this\n",
        "later)."
      ],
      "metadata": {
        "id": "PjreJ420j1Lt"
      },
      "id": "PjreJ420j1Lt"
    },
    {
      "cell_type": "code",
      "source": [
        "class MarioNet(nn.Module):\n",
        "    \"\"\"mini CNN structure\n",
        "    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )\n",
        "\n",
        "        self.target = copy.deepcopy(self.online)\n",
        "\n",
        "        # Q_target parameters are frozen.\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)"
      ],
      "metadata": {
        "id": "TxskxPwWj2tO"
      },
      "id": "TxskxPwWj2tO",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.4 - REVIEW OF VALUE METHODS OF REINFORCEMENT LEARNING\n",
        "\n",
        "#### TD Estimate & TD Target\n",
        "\n",
        "Two values are involved in learning:\n",
        "\n",
        "**TD Estimate** - the predicted optimal $Q^*$ for a given state\n",
        "$s$\n",
        "\n",
        "\\begin{align}{TD}_e = Q_{online}^*(s,a)\\end{align}\n",
        "\n",
        "**TD Target** - aggregation of current reward and the estimated\n",
        "$Q^*$ in the next state $s'$\n",
        "\n",
        "\\begin{align}a' = argmax_{a} Q_{online}(s', a)\\end{align}\n",
        "\n",
        "\\begin{align}{TD}_t = r + \\gamma Q_{target}^*(s',a')\\end{align}\n",
        "\n",
        "Because we don’t know what next action $a'$ will be, we use the\n",
        "action $a'$ maximizes $Q_{online}$ in the next state\n",
        "$s'$.\n",
        "\n",
        "\n",
        "\n",
        "#### Updating the model\n",
        "\n",
        "As Mario samples inputs from his replay buffer, we compute $TD_t$\n",
        "and $TD_e$ and backpropagate this loss down $Q_{online}$ to\n",
        "update its parameters $\\theta_{online}$ ($\\alpha$ is the\n",
        "learning rate ``lr`` passed to the ``optimizer``)\n",
        "\n",
        "\\begin{align}\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)\\end{align}\n",
        "\n",
        "$\\theta_{target}$ does not update through backpropagation.\n",
        "Instead, we periodically copy $\\theta_{online}$ to\n",
        "$\\theta_{target}$\n",
        "\n",
        "\\begin{align}\\theta_{target} \\leftarrow \\theta_{online}\\end{align}\n"
      ],
      "metadata": {
        "id": "gpseKCemlRtV"
      },
      "id": "gpseKCemlRtV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 - LOG PROGRESS"
      ],
      "metadata": {
        "id": "N50PgZCCrePO"
      },
      "id": "N50PgZCCrePO"
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelSaver(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq, save_path, verbose=1):\n",
        "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def _init_callback(self):\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self):\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
        "            self.model.save(model_path)\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
        "            plt.clf()\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
        "            plt.legend()\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
      ],
      "metadata": {
        "id": "v0YtpvoYY3Wy"
      },
      "id": "v0YtpvoYY3Wy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''INITIALISE MODEL SAVER'''\n",
        "CHECKPOINT_DIR = './train/'\n",
        "LOG_DIR = './logs/'\n",
        "# Setup model saving callback\n",
        "callback = ModelSaver(check_freq=10000, save_path=CHECKPOINT_DIR)"
      ],
      "metadata": {
        "id": "GBhpkWjfo1Na"
      },
      "id": "GBhpkWjfo1Na",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - TRAIN THE MODEL\n",
        "\n"
      ],
      "metadata": {
        "id": "xgZVzlaRr1n4"
      },
      "id": "xgZVzlaRr1n4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.1 - Using PPO: A Policy Gradient Method"
      ],
      "metadata": {
        "id": "0WJGb4Y9pVff"
      },
      "id": "0WJGb4Y9pVff"
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_learning_rate = 0.00001\n",
        "ppo_num_steps = 512\n",
        "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate= ppo_learning_rate,\n",
        "            n_steps=ppo_num_steps)\n",
        "\n",
        "# Train the AI model, this is where the AI model starts to learn\n",
        "model.learn(total_timesteps=1000000, callback=callback)\n",
        "\n",
        "model.save('thisisatestmodel')\n",
        "\n",
        "# Load model\n",
        "model = PPO.load('./train/best_model_1000000')\n",
        "\n",
        "state = env.reset()\n",
        "# Start the game\n",
        "state = env.reset()\n",
        "# Loop through the game\n",
        "while True:\n",
        "\n",
        "    action, _ = model.predict(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    env.render()"
      ],
      "metadata": {
        "id": "xAedqWsEpb-d"
      },
      "id": "xAedqWsEpb-d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.2 - Using Double Deep Q Learning (DDQL)"
      ],
      "metadata": {
        "id": "YSoCyrm8prDp"
      },
      "id": "YSoCyrm8prDp"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir,\\\n",
        "              checkpoint = None, epsilon = 1, e_rate_decay = 9.9995e-1,\\\n",
        "                 e_rate_min = 0.1, batch_size = 32, gamma = 0.9,\\\n",
        "               learning_rate = 0.00025, optimiser = 'Adam', loss = 'L1')\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "episodes = 40000\n",
        "for e in range(episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    while True:\n",
        "\n",
        "        # Run agent on the state\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # Agent performs action\n",
        "        next_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # Remember\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # Learn\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # Logging\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if e % 20 == 0:\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
      ],
      "metadata": {
        "id": "9VsomwJsY3Yp"
      },
      "id": "9VsomwJsY3Yp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXTRA TO IMPLEMENT LATER???"
      ],
      "metadata": {
        "id": "prRt5hmnqFsF"
      },
      "id": "prRt5hmnqFsF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the environment with RecordEpisodeStatistics\n",
        "env = RecordEpisodeStatistics(env)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.select_action(observation)\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "\n",
        "\n",
        "    # Wrap the environment with the RecordVideo wrapper\n",
        "env = RecordVideo(env, video_path='/path/to/output/video.mp4')\n",
        "\n",
        "# Run the agent in the environment\n",
        "for _ in range(1000):\n",
        "    action = env.action_space.sample()  # Replace with your agent's action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Close the environment, which finalizes and saves the video\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "Q55_eHgLXNQ3"
      },
      "id": "Q55_eHgLXNQ3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if gym.__version__ < '0.26':\n",
        "#     env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v1\", new_step_api=True)\n",
        "# else:\n",
        "#     env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v1\", render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "# env = JoypadSpace(\n",
        "#     env,\n",
        "#     [['right'],\n",
        "#     ['right', 'A']]\n",
        "# )\n",
        "\n",
        "# env = SkipFrame(env, skip=4)\n",
        "# env = GrayScaleObservation(env)\n",
        "# env = ResizeObservation(env, shape=84)\n",
        "# env = FrameStack(env, num_stack=4)\n",
        "\n",
        "# env.reset()\n",
        "\n",
        "# save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
        "# save_dir.mkdir(parents=True)\n",
        "\n",
        "# checkpoint = Path('checkpoints/2023-11-01T14-58-44/mario_net_0.chkpt')\n",
        "# mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, checkpoint=checkpoint)\n",
        "# mario.exploration_rate = mario.exploration_rate_min\n",
        "\n",
        "# logger = MetricLogger(save_dir)\n",
        "\n",
        "# episodes = 100\n",
        "\n",
        "# for e in range(episodes):\n",
        "\n",
        "#     state = env.reset()\n",
        "\n",
        "#     while True:\n",
        "\n",
        "#         env.render()\n",
        "\n",
        "#         action = mario.act(state)\n",
        "\n",
        "#         next_state, reward, done, info = env.step(action)\n",
        "\n",
        "#         mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "#         logger.log_step(reward, None, None)\n",
        "\n",
        "#         state = next_state\n",
        "\n",
        "#         if done or info['flag_get']:\n",
        "#             break\n",
        "\n",
        "#     logger.log_episode()\n",
        "\n",
        "#     if e % 20 == 0:\n",
        "#         logger.record(\n",
        "#             episode=e,\n",
        "#             epsilon=mario.exploration_rate,\n",
        "#             step=mario.curr_step\n",
        "#         )"
      ],
      "metadata": {
        "id": "J4oLr--XY0Xr"
      },
      "id": "J4oLr--XY0Xr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8eaVx9HY0aE"
      },
      "id": "d8eaVx9HY0aE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mVVC1ixsY0dA"
      },
      "id": "mVVC1ixsY0dA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pl-ct-LY02s"
      },
      "id": "3pl-ct-LY02s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manual preprocessing\n",
        "\n",
        "\n",
        "\n",
        "# MANUAL APPLICATIONS\n",
        "# class FrameStack:\n",
        "#     def __init__(self, env, num_stack):\n",
        "#         self.env = env\n",
        "#         self.num_stack = num_stack\n",
        "#         self.frames = deque(maxlen=num_stack)\n",
        "\n",
        "#     def reset(self):\n",
        "#         obs = self.env.reset()\n",
        "#         for _ in range(self.num_stack):\n",
        "#             self.frames.append(obs)\n",
        "#         return np.stack(self.frames, axis=0)\n",
        "\n",
        "#     def step(self, action):\n",
        "#         obs, reward, done, info = self.env.step(action)\n",
        "#         self.frames.append(obs)\n",
        "#         return np.stack(self.frames, axis=0), reward, done, info\n",
        "\n",
        "# class GrayScaleObservation(gym.ObservationWrapper):\n",
        "#     def __init__(self, env):\n",
        "#         super().__init__(env)\n",
        "#         obs_shape = self.observation_space.shape[:2]\n",
        "#         self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "#     def permute_orientation(self, observation):\n",
        "#         # permute [H, W, C] array to [C, H, W] tensor\n",
        "#         observation = np.transpose(observation, (2, 0, 1))\n",
        "#         observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "#         return observation\n",
        "\n",
        "#     def observation(self, observation):\n",
        "#         observation = self.permute_orientation(observation)\n",
        "#         transform = T.Grayscale()\n",
        "#         observation = transform(observation)\n",
        "#         return observation\n",
        "\n",
        "\n",
        "# class ResizeObservation(gym.ObservationWrapper):\n",
        "#     def __init__(self, env, shape):\n",
        "#         super().__init__(env)\n",
        "#         if isinstance(shape, int):\n",
        "#             self.shape = (shape, shape)\n",
        "#         else:\n",
        "#             self.shape = tuple(shape)\n",
        "\n",
        "#         obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "#         self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "#     def observation(self, observation):\n",
        "#         transforms = T.Compose(\n",
        "#             [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "#         )\n",
        "#         observation = transforms(observation).squeeze(0)\n",
        "#         return observation"
      ],
      "metadata": {
        "id": "O64j2HLFnOcv"
      },
      "id": "O64j2HLFnOcv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}